{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\_dev\\spark-3.5.1-hadoop3\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sparksession object\n",
    "# and providing appName\n",
    "spark = SparkSession.builder.appName(\"QA_SCD2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compare two dataframes\n",
    "# test functions\n",
    "\n",
    "\n",
    "def test_SrcExceptTarget(source, target, ac, failed_flag, extra=\"\"):\n",
    "    if source.exceptAll(target).count() != 0:\n",
    "        print(\"Test failed: records in source that are not in target - \", ac, extra)\n",
    "        source.exceptAll(target).display()\n",
    "        failed_flag = 1\n",
    "    return failed_flag\n",
    "\n",
    "\n",
    "def test_TargetExceptSrc(source, target, ac, failed_flag, extra=\"\"):\n",
    "    if target.exceptAll(source).count() != 0:\n",
    "        print(\"Test failed: records in target that are not in source - \", ac, extra)\n",
    "        target.exceptAll(source).display()\n",
    "        failed_flag = 1\n",
    "    return failed_flag\n",
    "\n",
    "\n",
    "def test_distinctTarget(source, target, ac, failed_flag, id):\n",
    "    src = source.select(col(id).alias(\"id\"))\n",
    "    joined = src.join(target, src.id == target[id], \"inner\")\n",
    "    if joined.count() != joined.dropDuplicates().count:\n",
    "        print(\"Test failed: duplicate target records - \", ac)\n",
    "        failed_flag = 1\n",
    "    return failed_flag\n",
    "\n",
    "\n",
    "def test_countSourceTarget(source, target, ac, failed_flag):\n",
    "    if target.count() != source.count():\n",
    "        print(\"Test failed: incorrect number of records - \", ac)\n",
    "        print(\"Distinct records in source: \", source.count())\n",
    "        print(\"Distinct records in curated: \", target.count())\n",
    "        failed_flag = 1\n",
    "    return failed_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expected_source(source, scd2_cols, ID_COL=\"pros_id\"):\n",
    "    SEPARATOR = \"Â¿\"\n",
    "    FIRST = \"First\"\n",
    "    MIDDLE = \"Middle\"\n",
    "    LAST = \"Last\"\n",
    "    CONCAT_COL = \"concat\"\n",
    "    RANK_COL = \"rank\"\n",
    "    CONSECUTIVE_COL = \"consecutive\"\n",
    "    START_AT_COL = \"start_at\"\n",
    "    START_AT_EARLIEST_COL = \"start_at_earliest\"\n",
    "\n",
    "    windowPartition = Window.partitionBy(ID_COL).orderBy([ID_COL, START_AT_COL])\n",
    "    # https://stackoverflow.com/questions/74942562/concat-ws-and-coalesce-in-pyspark\n",
    "    source = (\n",
    "        source.withColumn(\n",
    "            CONCAT_COL,\n",
    "            concat_ws(SEPARATOR, *scd2_cols),\n",
    "        )\n",
    "        .withColumn(\n",
    "            RANK_COL,\n",
    "            when(\n",
    "                (col(CONCAT_COL) != lag(CONCAT_COL).over(windowPartition))\n",
    "                & (col(CONCAT_COL) == lead(CONCAT_COL).over(windowPartition)),\n",
    "                lit(FIRST),\n",
    "            )\n",
    "            .when(\n",
    "                (col(CONCAT_COL) == lag(CONCAT_COL).over(windowPartition))\n",
    "                & (col(CONCAT_COL) == lead(CONCAT_COL).over(windowPartition)),\n",
    "                lit(MIDDLE),\n",
    "            )\n",
    "            .when(\n",
    "                (col(CONCAT_COL) == lag(CONCAT_COL).over(windowPartition))\n",
    "                & (col(CONCAT_COL) != lead(CONCAT_COL).over(windowPartition)),\n",
    "                lit(LAST),\n",
    "            )\n",
    "            .otherwise(lit(\"\")),\n",
    "        )\n",
    "        .withColumn(\n",
    "            CONSECUTIVE_COL,\n",
    "            when(\n",
    "                (col(CONCAT_COL) == lag(CONCAT_COL).over(windowPartition))\n",
    "                | (col(CONCAT_COL) == lead(CONCAT_COL).over(windowPartition)),\n",
    "                lit(True),\n",
    "            ).otherwise(lit(False)),\n",
    "        )\n",
    "        .withColumn(  # find earliest start date\n",
    "            START_AT_EARLIEST_COL,\n",
    "            when(\n",
    "                col(RANK_COL) == LAST,\n",
    "                min(col(START_AT_COL)).over(\n",
    "                    (\n",
    "                        Window.partitionBy(\n",
    "                            [ID_COL, CONCAT_COL, CONSECUTIVE_COL]\n",
    "                        ).orderBy([ID_COL, START_AT_COL])\n",
    "                    )\n",
    "                ),\n",
    "            ).otherwise(lit(None)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # source.display()\n",
    "\n",
    "    # update start_at with earliest start date when rank = last\n",
    "    source = source.withColumn(\n",
    "        START_AT_COL,\n",
    "        when(\n",
    "            col(START_AT_EARLIEST_COL).isNotNull(), col(START_AT_EARLIEST_COL)\n",
    "        ).otherwise(col(START_AT_COL)),\n",
    "    )\n",
    "\n",
    "    # drop rows with rank = first or middle\n",
    "    source = source.filter((col(RANK_COL) != MIDDLE) & (col(RANK_COL) != FIRST))\n",
    "\n",
    "    # drop helper columns such as concat, rank, consecutive, and start_at_earliest\n",
    "    source = source.drop(CONCAT_COL, RANK_COL, CONSECUTIVE_COL, START_AT_EARLIEST_COL)\n",
    "\n",
    "    return source.orderBy(ID_COL, START_AT_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename source table's columns to match target table's columns\n",
    "def rename_source_cols(source_df, source_cols, target_cols):\n",
    "    if len(source_cols) != len(target_cols):\n",
    "        print(\"Column count mismatch\")\n",
    "        exit(1)\n",
    "\n",
    "    pairs = list(zip(source_cols, target_cols))\n",
    "\n",
    "    for p in pairs:\n",
    "        if p[0] != p[1]:\n",
    "            source_df = source_df.withColumnRenamed(*p)\n",
    "\n",
    "    return source_df\n",
    "\n",
    "\n",
    "# drop table's columns\n",
    "def drop_cols(df, cols):\n",
    "    for c in df.columns:\n",
    "        if c not in cols:\n",
    "            # print(\"dropping\", c)\n",
    "            df = df.drop(c)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersections(list, lists):\n",
    "\n",
    "    maxdate = date(datetime.max.year, 1, 1)\n",
    "\n",
    "    # collect all points first into a set and the into a sorted sequence\n",
    "    breaks = set()\n",
    "    for l in lists:\n",
    "        breaks.update(*l)\n",
    "    # sort None values to the end\n",
    "    breaks = sorted(breaks, key=lambda x: (x or maxdate))\n",
    "\n",
    "    # print(\"breaks\", breaks)\n",
    "\n",
    "    intersections = []\n",
    "    index = 0\n",
    "    # For each interval\n",
    "    for start, end in list:\n",
    "        if end is not None:\n",
    "            # Advance b until it falls into this interval:\n",
    "            while breaks[index] <= start:\n",
    "                index += 1\n",
    "            # Now collect all sub-intervals from\n",
    "            while index < len(breaks) and (\n",
    "                breaks[index] <= end if breaks[index] is not None else False\n",
    "            ):\n",
    "                intersections.append([start, breaks[index]])\n",
    "                start = breaks[index]\n",
    "                index += 1\n",
    "        elif end is None:  # start 1, end None\n",
    "            # Advance b until it falls into this interval:\n",
    "            while breaks[index] <= start:\n",
    "                index += 1\n",
    "                if breaks[index] is None:\n",
    "                    intersections.append([start, breaks[index]])\n",
    "                    return intersections\n",
    "\n",
    "            # Now collect all sub-intervals from\n",
    "            while index < len(breaks) and breaks[index] is not None:\n",
    "                intersections.append([start, breaks[index]])\n",
    "                start = breaks[index]\n",
    "                index += 1\n",
    "\n",
    "            if breaks[index] is None:\n",
    "                intersections.append([start, breaks[index]])\n",
    "                return intersections\n",
    "\n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = namedtuple(\"NT\", [\"id\", \"start_date\", \"end_date\"])\n",
    "\n",
    "\n",
    "def find_date_intersections(daterange, lists):\n",
    "    # collect all endpoints first into a set and the into a sorted sequence\n",
    "    breaks = set()\n",
    "    for l in lists:\n",
    "        for x in l:\n",
    "            breaks.add(x.start_date)\n",
    "            breaks.add(x.end_date)\n",
    "\n",
    "    maxdate = date(datetime.max.year, 1, 1)\n",
    "    breaks = sorted(breaks, key=lambda x: (x or maxdate))\n",
    "    # print(breaks)\n",
    "\n",
    "    intersections: list[NT] = []\n",
    "    index = 0\n",
    "    for id, start, end in daterange:\n",
    "        # print(id, start, end)\n",
    "        if end is not None:\n",
    "            # Advance b until it falls into this interval:\n",
    "            while breaks[index] <= start:\n",
    "                index += 1\n",
    "            # Now collect all sub-intervals from\n",
    "            while index < len(breaks) and (\n",
    "                breaks[index] <= end if breaks[index] is not None else False\n",
    "            ):\n",
    "                intersections.append(NT(id, start, breaks[index]))\n",
    "                start = breaks[index]\n",
    "                index += 1\n",
    "        elif end is None:\n",
    "            # Advance b until it falls into this interval:\n",
    "            while breaks[index] <= start:\n",
    "                index += 1\n",
    "                if breaks[index] is None:\n",
    "                    intersections.append(NT(id, start, breaks[index]))\n",
    "                    return intersections\n",
    "\n",
    "            # Now collect all sub-intervals from\n",
    "            while index < len(breaks) and breaks[index] is not None:\n",
    "                intersections.append(NT(id, start, breaks[index]))\n",
    "                start = breaks[index]\n",
    "                index += 1\n",
    "\n",
    "            if breaks[index] is None:\n",
    "                intersections.append(NT(id, start, breaks[index]))\n",
    "                return intersections\n",
    "\n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target test data, Gperson\n",
    "schema = StructType(  # Define the whole schema within a StructType\n",
    "    [\n",
    "        StructField(\"person_id\", DecimalType(25, 0), True),\n",
    "        StructField(\"Surname_cache\", StringType(), True),\n",
    "        StructField(\"__START_AT\", DateType(), True),\n",
    "        StructField(\"__END_AT\", DateType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        Decimal(\"1\"),\n",
    "        \"t\",\n",
    "        datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-01-02\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"1\"),\n",
    "        \"te\",\n",
    "        datetime.strptime(\"2023-01-03\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-01-04\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"1\"),\n",
    "        \"test\",\n",
    "        datetime.strptime(\"2023-01-05\", \"%Y-%m-%d\"),\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"2\"),\n",
    "        \"tes\",\n",
    "        datetime.strptime(\"2023-02-05\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-02-09\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"2\"),\n",
    "        \"test\",\n",
    "        datetime.strptime(\"2023-02-10\", \"%Y-%m-%d\"),\n",
    "        None,\n",
    "    ),\n",
    "]\n",
    "\n",
    "gperson = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data for Gpid\n",
    "schema = StructType(  # Define the whole schema within a StructType\n",
    "    [\n",
    "        StructField(\"Id\", DecimalType(25, 0), True),\n",
    "        StructField(\"WId\", DecimalType(), True),\n",
    "        StructField(\"IDNumber\", DecimalType(), True),\n",
    "        StructField(\"START_AT\", DateType(), True),\n",
    "        StructField(\"END_AT\", DateType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        Decimal(\"1\"),\n",
    "        Decimal(\"1\"),\n",
    "        Decimal(\"123\"),\n",
    "        datetime.strptime(\"2023-01-03\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-01-04\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"2\"),\n",
    "        Decimal(\"1\"),\n",
    "        Decimal(\"456\"),\n",
    "        datetime.strptime(\"2023-01-06\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-01-10\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"3\"),\n",
    "        Decimal(\"1\"),\n",
    "        Decimal(\"789\"),\n",
    "        datetime.strptime(\"2023-01-12\", \"%Y-%m-%d\"),\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"4\"),\n",
    "        Decimal(\"2\"),\n",
    "        Decimal(\"888\"),\n",
    "        datetime.strptime(\"2023-02-01\", \"%Y-%m-%d\"),\n",
    "        datetime.strptime(\"2023-02-07\", \"%Y-%m-%d\"),\n",
    "    ),\n",
    "    (\n",
    "        Decimal(\"5\"),\n",
    "        Decimal(\"2\"),\n",
    "        Decimal(\"999\"),\n",
    "        datetime.strptime(\"2023-02-20\", \"%Y-%m-%d\"),\n",
    "        None,\n",
    "    ),\n",
    "]\n",
    "\n",
    "gpid = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Select records from gperson where 'Id' begins with 15\n",
    "# Left join:\n",
    "# - gperson.Id = gpid.WId\n",
    "# Drop duplicate rows.\n",
    "\n",
    "gperson_gpid = gperson.join(\n",
    "    gpid, gperson.person_id == gpid.WId, \"left\"\n",
    ").dropDuplicates()\n",
    "\n",
    "# gperson_gpid.printSchema()\n",
    "# gperson_gpid.display()\n",
    "\n",
    "\n",
    "gperson_gpid = gperson_gpid.orderBy(\n",
    "    \"person_id\", \"Surname_cache\", \"__START_AT\", \"START_AT\"\n",
    ")\n",
    "\n",
    "gperson_gpid = gperson_gpid.withColumn(\n",
    "    \"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id()))\n",
    ")\n",
    "\n",
    "gperson_gpid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_intersections = []\n",
    "\n",
    "for row in gperson_gpid.collect():\n",
    "    person_start_at = row[\"__START_AT\"]\n",
    "    person_end_at = row[\"__END_AT\"]\n",
    "    person_id = row[\"row_idx\"]\n",
    "    nt1 = NT(person_id, person_start_at, person_end_at)\n",
    "    \n",
    "    gpid_start_at = row.START_AT\n",
    "    gpid_end_at = row.END_AT\n",
    "    \n",
    "    nt2 = NT(person_id, gpid_start_at, gpid_end_at)\n",
    "    \n",
    "    a = [nt1]\n",
    "    b = [nt2]\n",
    "\n",
    "    lists = [a, b]\n",
    "    intersections = find_date_intersections(a, lists)\n",
    "\n",
    "    list_of_intersections.append(intersections)\n",
    "\n",
    "    print(intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = spark.createDataFrame([(l,) for l in list_of_intersections], [\"Intersections\"])\n",
    "\n",
    "abc = []\n",
    "for l in list_of_intersections:\n",
    "    for i in l:\n",
    "        # print(i)\n",
    "        abc.append(i)\n",
    "\n",
    "b = spark.createDataFrame(abc, ['row_idx', 'start_date_new', 'end_date_new'])\n",
    "\n",
    "b.show()\n",
    "\n",
    "gperson_gpid  = gperson_gpid.join(b, gperson_gpid.row_idx == b.row_idx, \"left\")\n",
    "\n",
    "gperson_gpid.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
